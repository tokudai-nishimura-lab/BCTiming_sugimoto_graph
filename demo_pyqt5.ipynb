{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "from transformers import HubertModel, Wav2Vec2FeatureExtractor\n",
    "import time\n",
    "import threading\n",
    "import queue\n",
    "import random\n",
    "import soundfile as sf\n",
    "\n",
    "import sys\n",
    "from PyQt5.QtWidgets import QApplication, QWidget, QVBoxLayout\n",
    "from PyQt5.QtChart import QChart, QChartView, QLineSeries, QValueAxis, QLegend\n",
    "from PyQt5.QtCore import Qt, QTimer, QCoreApplication\n",
    "from PyQt5.QtGui import QPainter, QFont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 入力デバイスを確認、選択する関数\n",
    "def select_input_device():\n",
    "    devices = sd.query_devices()\n",
    "    input_devices = [d for d in devices if d['max_input_channels'] > 0]\n",
    "    \n",
    "    print(\"Available input devices:\")\n",
    "    for i, device in enumerate(input_devices):\n",
    "        print(f\"{i}: {device['name']}\")\n",
    "    \n",
    "    selection = int(input(\"Select input device number: \"))\n",
    "    return input_devices[selection]['index']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuBERTモデルとFeature Extractorの準備\n",
    "MODEL_ID = \"rinna/japanese-hubert-base\"\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(MODEL_ID)\n",
    "hubert_model = HubertModel.from_pretrained(MODEL_ID).to(device)\n",
    "\n",
    "# 使用する中間層のインデックスを指定(0~11)\n",
    "# LAYER_INDEX = 6  # ７番目\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model_and_params():\n",
    "    \"\"\"\n",
    "    ユーザーに入力音声の長さを問い合わせ、対応するモデルのパスとwindow_sizeを返す関数\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (model_path, window_size) - 選択されたモデルのパスと対応するwindow_size\n",
    "    \"\"\"\n",
    "    # 利用可能なモデルとその対応する音声長（秒）\n",
    "    available_models = {\n",
    "        0.5: {\"model\": \"0.5s_CSJmodel.pth\", \"window_size\": 0.5},\n",
    "        1.0: {\"model\": \"1s_CSJmodel.pth\", \"window_size\": 1.0},\n",
    "        2.0: {\"model\": \"2s_CSJmodel.pth\", \"window_size\": 2.0}\n",
    "    }\n",
    "    \n",
    "    # ユーザーに入力を求める\n",
    "    while True:\n",
    "        try:\n",
    "            duration_ms = int(input(\"入力する音声の長さを指定してください(ms): \"))\n",
    "            duration_s = duration_ms / 1000  # ミリ秒から秒に変換\n",
    "            \n",
    "            # 利用可能なモデルのリストから最も近いものを選択\n",
    "            if duration_s in available_models:\n",
    "                selected_params = available_models[duration_s]\n",
    "                model_path = f\"model/{selected_params['model']}\"\n",
    "                window_size = selected_params['window_size']\n",
    "                print(f\"選択されたモデル: {model_path}\")\n",
    "                print(f\"モデルへの入力音声長: {window_size}秒\")\n",
    "                return model_path, window_size\n",
    "            else:\n",
    "                # 最も近い値を探す\n",
    "                closest_duration = min(available_models.keys(), key=lambda x: abs(x - duration_s))\n",
    "                selected_params = available_models[closest_duration]\n",
    "                model_path = f\"model/{selected_params['model']}\"\n",
    "                window_size = selected_params['window_size']\n",
    "                print(f\"指定された長さに最も近いモデル: {model_path} ({closest_duration}秒)\")\n",
    "                print(f\"window_size: {window_size}秒\")\n",
    "                confirm = input(\"このモデルとパラメータを使用しますか？ (y/n): \")\n",
    "                if confirm.lower() == 'y':\n",
    "                    return model_path, window_size\n",
    "                print(\"再度入力してください。\")\n",
    "        except ValueError:\n",
    "            print(\"数値を入力してください。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルのパス変数とwindow_sizeを設定\n",
    "saved_model_pth, window_size = select_model_and_params()\n",
    "print(f\"使用するモデルのパス: {saved_model_pth}\")\n",
    "print(f\"設定された入力音声長: {window_size}秒\")\n",
    "\n",
    "# 音声処理パラメータ\n",
    "sample_rate = 16000\n",
    "stride = 0.2  # 予測頻度(s)\n",
    "buffer_size = int(window_size * sample_rate)\n",
    "stride_size = int(stride * sample_rate)\n",
    "bc_thre  = 0.80 # 予測のしきい値"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTMモデルの準備\n",
    "input_dim = 768\n",
    "hidden_dim = 128\n",
    "output_dim = 1\n",
    "lstm_model = LSTMModel(input_dim, hidden_dim, output_dim).to(device)\n",
    "lstm_model.load_state_dict(torch.load(saved_model_pth, map_location=device))\n",
    "lstm_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def select_audio_files():\n",
    "    \"\"\"\n",
    "    ユーザーに使用する音声ファイルのディレクトリを選択してもらい、\n",
    "    選択したディレクトリ内のすべてのWAVファイルを返す関数\n",
    "    \n",
    "    Returns:\n",
    "        list: 選択したディレクトリ内のすべての音声ファイルのパスのリスト\n",
    "    \"\"\"\n",
    "    # 音声ファイルのディレクトリを選択\n",
    "    base_dir = \"bc_wav\"\n",
    "    # 利用可能なディレクトリを取得\n",
    "    available_dirs = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
    "    \n",
    "    if not available_dirs:\n",
    "        print(f\"警告: {base_dir}ディレクトリ内にサブディレクトリが見つかりませんでした。\")\n",
    "        return []\n",
    "    \n",
    "    print(\"\\n利用可能な音声ディレクトリ:\")\n",
    "    for i, dir_name in enumerate(available_dirs):\n",
    "        print(f\"{i}: {dir_name}\")\n",
    "    \n",
    "    # ユーザーにディレクトリを選択してもらう\n",
    "    while True:\n",
    "        try:\n",
    "            selection = int(input(\"\\n音声ディレクトリを選択してください (番号): \"))\n",
    "            if 0 <= selection < len(available_dirs):\n",
    "                selected_dir = available_dirs[selection]\n",
    "                break\n",
    "            else:\n",
    "                print(f\"0から{len(available_dirs)-1}までの番号を入力してください。\")\n",
    "        except ValueError:\n",
    "            print(\"数値を入力してください。\")\n",
    "    \n",
    "    # 選択したディレクトリ内のすべてのWAVファイルを取得\n",
    "    selected_dir_path = os.path.join(base_dir, selected_dir)\n",
    "    wav_files = glob.glob(f\"{selected_dir_path}/*.wav\")\n",
    "    \n",
    "    # 相対パスに変換\n",
    "    bc_audio_files = [f\"bc_wav/{selected_dir}/{os.path.basename(f)}\" for f in wav_files]\n",
    "    \n",
    "    print(f\"\\n選択された音声ディレクトリ: {selected_dir}\")\n",
    "    print(f\"読み込まれた音声ファイル数: {len(bc_audio_files)}\")\n",
    "    if bc_audio_files:\n",
    "        print(\"音声ファイル:\")\n",
    "        for file in bc_audio_files:\n",
    "            print(f\"  - {file}\")\n",
    "    else:\n",
    "        print(f\"警告: 選択されたディレクトリ {selected_dir_path} にWAVファイルが見つかりませんでした。\")\n",
    "    \n",
    "    return bc_audio_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 次に音声ファイル選択関数を使用\n",
    "bc_audio_files = select_audio_files()\n",
    "\n",
    "print(\"\\n設定の概要:\")\n",
    "print(f\"使用するモデルのパス: {saved_model_pth}\")\n",
    "print(f\"設定されたwindow_size: {window_size}秒\")\n",
    "print(f\"使用する音声ファイル: {len(bc_audio_files)}個\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# バックチャネル音声ファイルの読み込み\n",
    "bc_audios = []\n",
    "bc_srs = []\n",
    "for file in bc_audio_files:\n",
    "    audio, sr = sf.read(file)\n",
    "    bc_audios.append(audio)\n",
    "    bc_srs.append(sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 相槌の再生 (関数名を英語に変更)\n",
    "def play_backchanneling():\n",
    "    global last_backchanneling_time, suppression_duration # グローバル変数を参照\n",
    "\n",
    "    current_time = time.time() # 現在時刻を取得\n",
    "\n",
    "    # 前回相槌再生時刻から抑制時間以上経過しているか確認\n",
    "    if current_time - last_backchanneling_time >= suppression_duration:\n",
    "        # ランダムに相槌音声を選択\n",
    "        idx = random.randint(0, len(bc_audios) - 1)\n",
    "        audio = bc_audios[idx]\n",
    "        sr = bc_srs[idx]\n",
    "        sd.play(audio, blocking=False, samplerate=sr)\n",
    "        last_backchanneling_time = current_time # 最後に相槌を再生した時刻を更新\n",
    "        print(\"相槌再生\") # デバッグ用メッセージ (動作確認用)\n",
    "    else:\n",
    "        print(\"相槌抑制中\") # デバッグ用メッセージ (動作確認用)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### カーネルは必ず再起動してから実行！\n",
    "#### 連続相槌の抑制時間は0.5-1秒が目安"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# グローバル変数\n",
    "time_data = []\n",
    "confidence_data = []\n",
    "last_prediction_time = 0\n",
    "audio_buffer = np.zeros(buffer_size)\n",
    "start_time = None\n",
    "display_duration = 20 # グラフの表示時間（秒）\n",
    "\n",
    "last_backchanneling_time = 0  # 最後に相槌を再生した時刻を記録する変数\n",
    "suppression_duration = 1  # 相槌音声再生後の抑制時間（秒）【ユーザー指定: 0.5秒】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# グラフのリセット\n",
    "def reset_graph():\n",
    "    global time_data, confidence_data, start_time\n",
    "    time_data = []\n",
    "    confidence_data = []\n",
    "    start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# グラフの更新\n",
    "def update_graph():\n",
    "    global series, time_data, confidence_data, axis_x\n",
    "\n",
    "    current_time = time.time() - start_time\n",
    "\n",
    "    # 表示範囲内のデータのみ表示\n",
    "    time_data_filtered = [t for t in time_data if t <= current_time]\n",
    "    confidence_data_filtered = confidence_data[:len(time_data_filtered)]\n",
    "\n",
    "    series.clear()\n",
    "    for t, c in zip(time_data_filtered, confidence_data_filtered):\n",
    "        series.append(t, c)\n",
    "\n",
    "    # x軸の範囲を更新\n",
    "    axis_x.setRange(0, display_duration)\n",
    "\n",
    "    # グラフが限界に達したらリセット\n",
    "    if current_time >= display_duration:\n",
    "        reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 音声入力と予測の処理\n",
    "def process_audio(indata, frames, time_info, status):\n",
    "    global audio_buffer, last_prediction_time, time_data, confidence_data, start_time\n",
    "\n",
    "    if start_time is None:  # start_time が None の場合のみ初期化\n",
    "        start_time = time.time() - stride # 最初の推論結果がグラフの開始になるように調整\n",
    "\n",
    "    audio_buffer = np.roll(audio_buffer, -len(indata))\n",
    "    audio_buffer[-len(indata):] = indata[:, 0]\n",
    "\n",
    "    current_time = time.time()\n",
    "\n",
    "    if current_time - last_prediction_time >= stride and np.any(audio_buffer):  # より緩和されたチェック\n",
    "        inputs = feature_extractor(audio_buffer, sampling_rate=sample_rate, return_tensors=\"pt\").input_values\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            wav2vec_output = hubert_model(inputs).last_hidden_state\n",
    "\n",
    "        lstm_input = wav2vec_output\n",
    "        prediction = lstm_model(lstm_input)\n",
    "\n",
    "        result = torch.sigmoid(prediction).item()\n",
    "        confidence = result\n",
    "\n",
    "        if result > bc_thre:\n",
    "            threading.Thread(target=play_backchanneling).start()\n",
    "\n",
    "        last_prediction_time = current_time\n",
    "\n",
    "        time_data.append(current_time - start_time)\n",
    "        confidence_data.append(confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# グラフの初期化 (修正後)\n",
    "def init_graph():\n",
    "    global chart, series, threshold_series, layout, axis_x, axis_y\n",
    "\n",
    "    chart = QChart()\n",
    "    series = QLineSeries()\n",
    "    threshold_series = QLineSeries()\n",
    "    chart.addSeries(series)\n",
    "    chart.addSeries(threshold_series)\n",
    "\n",
    "    # フォント設定\n",
    "    font = QFont() # QFontオブジェクトを作成\n",
    "    font.setPointSize(30) # フォントサイズを12ポイントに設定 (調整可能)\n",
    "\n",
    "    # タイトルフォント設定\n",
    "    chart_title_font = QFont() # タイトル用に別のQFontオブジェクトを作成しても良い\n",
    "    chart_title_font.setPointSize(40) # タイトルは少し大きめに設定 (調整可能)\n",
    "    chart_title_font.setBold(True) # タイトルを太字に (必要に応じて)\n",
    "    chart.setTitle(\"相槌生成タイミング予測システム\")\n",
    "    chart.setTitleFont(chart_title_font) # タイトルにフォントを適用\n",
    "\n",
    "\n",
    "    axis_x = QValueAxis()\n",
    "    axis_x.setTitleText(\"経過時間\")\n",
    "    axis_x.setRange(0, display_duration)  # 初期表示範囲を設定\n",
    "\n",
    "    # X軸タイトルフォント設定\n",
    "    axis_x_font = QFont()\n",
    "    axis_x_font.setPointSize(30) # 軸タイトルフォントサイズ設定 (調整可能)\n",
    "    axis_x.setTitleFont(axis_x_font) # X軸タイトルにフォントを適用\n",
    "    axis_x.setTickType(QValueAxis.TicksDynamic)\n",
    "    axis_x.setTickInterval(1.0)  # strideに合わせて格子線を表示\n",
    "    chart.addAxis(axis_x, Qt.AlignBottom)\n",
    "    series.attachAxis(axis_x)\n",
    "    threshold_series.attachAxis(axis_x)\n",
    "\n",
    "\n",
    "    axis_y = QValueAxis()\n",
    "    axis_y.setTitleText(\"期待値\")\n",
    "    axis_y.setRange(0, 1)\n",
    "    axis_y.setTickCount(11)\n",
    "\n",
    "    # Y軸タイトルフォント設定\n",
    "    axis_y_font = QFont()\n",
    "    axis_y_font.setPointSize(30) # 軸タイトルフォントサイズ設定 (調整可能)\n",
    "    axis_y.setTitleFont(axis_y_font) # Y軸タイトルにフォントを適用\n",
    "    chart.addAxis(axis_y, Qt.AlignLeft)\n",
    "    series.attachAxis(axis_y)\n",
    "    threshold_series.attachAxis(axis_y)\n",
    "\n",
    "\n",
    "    # 閾値の線を初期化時に一度だけ描画\n",
    "    threshold_series.append(0, bc_thre)\n",
    "    threshold_series.append(display_duration, bc_thre)\n",
    "\n",
    "    chart_view = QChartView(chart)\n",
    "    chart_view.setRenderHint(QPainter.Antialiasing)\n",
    "\n",
    "    layout.addWidget(chart_view)\n",
    "\n",
    "    # 凡例の追加\n",
    "    legend = chart.legend()\n",
    "    legend.setVisible(True)\n",
    "    legend.setAlignment(Qt.AlignTop)\n",
    "\n",
    "    # 凡例フォント設定\n",
    "    legend_font = QFont()\n",
    "    legend_font.setPointSize(30) # 凡例フォントサイズ設定 (調整可能)\n",
    "    legend.setFont(legend_font) # 凡例にフォントを適用\n",
    "\n",
    "    series.setName(\"期待値\")\n",
    "    threshold_series.setName(\"閾値\")\n",
    "\n",
    "    reset_graph()  # グラフの初期化時にリセット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# メイン処理\n",
    "if __name__ == \"__main__\":\n",
    "    app = QApplication(sys.argv)\n",
    "    window = QWidget()\n",
    "    window.resize(1200, 900)\n",
    "    layout = QVBoxLayout()\n",
    "    window.setLayout(layout)\n",
    "\n",
    "    init_graph()\n",
    "\n",
    "    device_index = select_input_device()\n",
    "    start_time = time.time()\n",
    "\n",
    "    q = queue.Queue()\n",
    "    try:\n",
    "        with sd.InputStream(\n",
    "            device=device_index,\n",
    "            channels=1,\n",
    "            samplerate=sample_rate,\n",
    "            callback=process_audio,\n",
    "            blocksize=stride_size,  # ブロックサイズを明示的に設定\n",
    "            latency='low'  # レイテンシーを低く設定\n",
    "            ):\n",
    "            \n",
    "            print(\"Recording... Press Ctrl+C to stop.\")\n",
    "            \n",
    "            timer = QTimer()\n",
    "            timer.timeout.connect(update_graph)\n",
    "            timer.start(100)\n",
    "\n",
    "            window.show()\n",
    "            app.exec_()\n",
    "            q.get()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Recording stopped.\")\n",
    "    finally:\n",
    "        QCoreApplication.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
